{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gxYksPamz4sx",
        "ykrhdPpbqwcl",
        "vh-BE7jKrNMd",
        "PaUyfXZCbfnU",
        "KixS_2xuHGYD",
        "lb5zRxN-yeTC",
        "c_Vbt1USQTi-",
        "VTaNR59rymiZ",
        "lMzXPxVA_d8p",
        "SpzSUY0I8YqD",
        "Aq5YiOtuZ8ge"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "## **Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Semana: Autoencoders y Técnicas de Conglomerados (Clustering)**"
      ],
      "metadata": {
        "id": "N9T_zdfbfcBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción**"
      ],
      "metadata": {
        "id": "gxYksPamz4sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sabemos que kMeans es un método de clasificación (clustering) no supervisado que nos ayuda a encontrar subgrupos de datos similares dentro de un conjunto de datos. Veamos ahora el efecto que puedan tener diferentes técnicas de reducción de dimensionalidad, Autoencoders y PCA, en la calidad de dichos subgrupos."
      ],
      "metadata": {
        "id": "hjJCCNfdNHy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La página inicial de Kaggle de donde proceden dichos datos se desactualizó, pero mientras la acutalizan, el archivo de los datos estará en Canvas.\n",
        "\n",
        "El archivo consta de 15,000 registros con 9 factores para un problema de ambiente de trabajo en una organización (entre ellos el problema de rotación de personal como ya estudiamos en semanas anteriores) con variables numéricas y categóricas.\n",
        "\n",
        "La idea es usar técnicas de Autoencoders y PCA, en combinación con el método de KMeans, para encontrar subgrupos o cúmulos de personas que nos den información sobre el ambiente de trabajo dentro de dicha organización.\n",
        "\n",
        "Las técnicas que aplicaremos son técnicas no supervisadas, ya que no tenemos la información en relación a un problema de clasificación o regresión.\n",
        "\n"
      ],
      "metadata": {
        "id": "a5PApIU-SBY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así, usaremos técnicas de reducción de dimensionalidad para identificar subgrupos similares de empleados a través de información proporcionada por el archivo proveniente de recursos humanos. Primero vamos a identificar los subgrupos y después vamos predecir qué datos están en cada subgrupo.\n",
        "\n",
        "Para entender lo que nos dicen dichos subgrupos desde un punto de vista de negocio, necesitamos analizar dichos subrupos con respecto a los factores iniciales. Es decir, ¿qué tienen en común los empleados que están en un mismo subgrupo?\n",
        "\n",
        "Recuerda que la manera en que funciona el método del codo (elbow method) para seleccionar el mejor número de subrupos en la técnica de kMeans, es buscando la cantidad de cúmulos que nos lleve a la suma más baja de distancias al cuadrado (llamada \"inercia\"), es decir, donde la inercia no sufra cambios importantes al seguir aumentando la cantidad de cúmulos.\n"
      ],
      "metadata": {
        "id": "9ZyQksQa-Fus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjIqp7HGR-BT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# comentemos por el momento los posibles Warnings:\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funciones para gráficas de la última parte de esta atcividad.**"
      ],
      "metadata": {
        "id": "ykrhdPpbqwcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta celda contiene las funciones para generar varios de los\n",
        "# gráficos que requeriremos en esta actividad.\n",
        "\n",
        "# Definimos la función de gráficos con datos numéricos:\n",
        "\n",
        "def mi_scatterplotPCAEncoder(perfil_Zencoder, perfil_pca, fact1, fact2):\n",
        "\n",
        "  plt.figure(figsize=(15,10))\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,15))\n",
        "\n",
        "  sns.scatterplot(data=perfil_Zencoder,\n",
        "                  x= fact1,\n",
        "                  y= fact2,\n",
        "                  hue='autoencoder_clusters',\n",
        "                  s=85,\n",
        "                  alpha=0.4,\n",
        "                  palette='bright',\n",
        "                  ax=ax1\n",
        "                  ).set_title('(AutoEncoder) Cúmulos',fontsize=18)\n",
        "\n",
        "  sns.scatterplot(data=perfil_pca,\n",
        "                  x=fact1,\n",
        "                  y=fact2,\n",
        "                  hue='pca_clusters',\n",
        "                  s=85,\n",
        "                  alpha=0.6,\n",
        "                  palette='bright'\n",
        "                  ).set_title('(PCA) Cúmulos',fontsize=18)\n",
        "\n",
        "\n",
        "\n",
        "# Definimos la función de gráficos con variables categóricas:\n",
        "\n",
        "def mi_catplotPCAEncoder(perfil_Zencoder, perfil_pca, fact1, fact2):\n",
        "\n",
        "  plt.figure(figsize=(15,10))\n",
        "  fig = plt.subplots(1,2, figsize=(20,15))\n",
        "\n",
        "  ax1 = sns.catplot(data=perfil_Zencoder,\n",
        "                    x= fact1,\n",
        "                    y= fact2,\n",
        "                    hue='autoencoder_clusters',\n",
        "                    jitter=0.6,\n",
        "                    s=20,\n",
        "                    alpha=0.6,\n",
        "                    palette='bright',\n",
        "                    legend_out=True\n",
        "                    )\n",
        "\n",
        "  ax2 = sns.catplot(data=perfil_pca,\n",
        "                    x=fact1,\n",
        "                    y=fact2,\n",
        "                    hue='pca_clusters',\n",
        "                    jitter=0.6,\n",
        "                    s=20,\n",
        "                    alpha=0.6,\n",
        "                    palette='bright',\n",
        "                    legend_out=True\n",
        "                    )\n",
        "\n",
        "  ax1.fig.suptitle('(AutoEncoder) Cúmulos', fontsize=16,)\n",
        "  ax2.fig.suptitle('(PCA) Cúmulos', fontsize=16)\n",
        "  plt.close(1)\n",
        "  plt.close(2)"
      ],
      "metadata": {
        "id": "hTMnprddqvYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cargando la información**"
      ],
      "metadata": {
        "id": "vh-BE7jKrNMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos:\n",
        "\n",
        "data = pd.read_csv(\"hr_autoencoders.csv\",sep=',',header='infer')\n",
        "print(data.shape)\n",
        "data.head().T"
      ],
      "metadata": {
        "id": "PNinqvNp5cJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La información sobre cada variable es la siguiente:\n",
        "\n",
        "*   #### **satisfaction_level:** Nivel de satisfacción del empleado dentro de la organización en la última encuesta realizda. Escala de 0 a 1, donde 1 es lo mejor.\n",
        "\n",
        "*   #### **last_evaluation:** Calificación del desempeño del empleado en la última evaluación realizada por la empresa. Escala de 0 a 1, donde 1 es lo mejor.\n",
        "\n",
        "*   #### **number_project:** Número promedio de proyectos en los que usualmente está involucrado dentro de la empresa.\n",
        "\n",
        "*   #### **average_montly_hours:** Horas mensuales promedio de trabajo en la empresa.\n",
        "\n",
        "*   #### **time_spend_company:** Cantidad de años dentro de la empresa.\n",
        "\n",
        "*   #### **Work_accident:** Si ha tenido (1) o no (0) un accidente dentro de la empresa.\n",
        "\n",
        "*   #### **promotion_last_5years:** Si ha sido promovido (1) o no (0) dentro de la empresa en los últimos 5 años.\n",
        "\n",
        "*   #### **sales:** Área a la que pertenece dentro de la empresa: sales, technical, support, IT, product_mng, marketing, RandD, accounting, hr, management.\n",
        "\n",
        "*   #### **salary:** Nivel de salario del empleado: low, medium, high."
      ],
      "metadata": {
        "id": "Gpxk80sElFLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "5M_oKvv-5zqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las variables numéricas son: satisfaction_level, last_evaluation, number_project, average_montly_hours, time_spend_company.\n",
        "\n",
        "Las variables categóricas binarias son: Work_accident, promotion_last_5years.\n",
        "\n",
        "Varaible categórica nominal: sales.\n",
        "\n",
        "Variable categórica ordinal: salary.\n",
        "\n",
        "#### **Apliquemos las transformaciones correspondientes a cada una de estas variables**"
      ],
      "metadata": {
        "id": "sGQaZvJyrLRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_list = ['satisfaction_level', 'last_evaluation', 'number_project',\n",
        "           'average_montly_hours', 'time_spend_company']   # [0,1,2,3,4]\n",
        "\n",
        "cat_list = ['Work_accident','promotion_last_5years','sales','salary']   # [5,6,7,8]\n",
        "\n",
        "dftodos = data[num_list + cat_list]\n",
        "print(dftodos.shape)\n",
        "dftodos.head().T"
      ],
      "metadata": {
        "id": "9-yOF-Vwio_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Escalamos las variables numéricas antes de aplicar KMeans,\n",
        "# ya que este es muy sensible a la escala:\n",
        "\n",
        "X_Tnum = dftodos.copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(dftodos[num_list])\n",
        "X_Tnum[num_list] = scaler.transform(dftodos[num_list])\n",
        "\n",
        "df_Tnum = pd.DataFrame(X_Tnum, columns=dftodos.columns)\n",
        "\n",
        "df_Tnum.head().T"
      ],
      "metadata": {
        "id": "_ldM6ju4Ulfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La matriz de correlación de las varaibles numéricas nos indica que no se\n",
        "# observan correlaciones altas entre ellas, es decir, se tiene una\n",
        "# multicolinealidad baja:\n",
        "\n",
        "correlations = df_Tnum[num_list].corr()\n",
        "f, ax = plt.subplots(figsize = (6,6))\n",
        "sns.heatmap(correlations, annot = True);"
      ],
      "metadata": {
        "id": "F6R6xkH9iwKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KPrototype : KMeans para variables numéricas y categóricas.**\n"
      ],
      "metadata": {
        "id": "PaUyfXZCbfnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Recordemos que KMeans es una técnica para variables numéricas ya que está basada en la distancia (por ejemplo, euclideana) entre puntos. Para el caso de variables categóricas, dicha \"distancia\" será ahora la Moda y la similitud será la diferencia entre los distintos niveles.**\n",
        "\n",
        "#### **Para ello debemos usar la librería KModes para datos solamente categóricos, o bien KPrototype, para conjunto de datos mixtos (numéricos y categóricos).**\n",
        "\n",
        "#### **Toma en cuenta que debido al proceso de estar manejando variables numéricas y categóricas, el método de KPrototype puede tomar varios minutos en terminarse.**"
      ],
      "metadata": {
        "id": "j9Ck3dlZzAT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kmodes   # Debemos instalar KModes, que necesitaremos para KPrototype."
      ],
      "metadata": {
        "id": "74TTd-FGbx7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kmodes.kprototypes import KPrototypes"
      ],
      "metadata": {
        "id": "3hL3pjm5bi1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tnum.head().T"
      ],
      "metadata": {
        "id": "xIJCVvdzcLIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos la información de las variables categóricas:"
      ],
      "metadata": {
        "id": "CTMJOo5OyhkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tnum['Work_accident'].value_counts()"
      ],
      "metadata": {
        "id": "K8lYCqHdZcen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tnum['promotion_last_5years'].value_counts()"
      ],
      "metadata": {
        "id": "BOjvopwQZgbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tnum['sales'].value_counts()"
      ],
      "metadata": {
        "id": "jSNV0DxSZSag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tnum['salary'].value_counts()"
      ],
      "metadata": {
        "id": "jsD2VvBHzKUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Método de inicialización de KModes y KPrototype:**\n",
        "\n",
        "Existen varias técnicas de inicialización de las variables catagóricas, en particular usaremos la llamada \"Huang\", la cual selecciona los primeros \"K\" registros distintos para iniciar con \"K-Modes\". Recordemos que en KModes se utuliza la moda en lugar del promedio aritmético que se aplica en las numéricas.\n",
        "\n",
        "La \"distancia\" o \"diferencia\" entre un dato y un centroide será la suma de la diferencia de la parte numérica más la parte categórica. La parte numérica se calcula con la distancia euclideana usual sobre las coordenadas numéricas. Y la parte categórica mide la distancia de un dato a un centroide, como la cantidad de coordenadas categóricas diferentes dentre el dato y el centroide.\n",
        "\n",
        "Las variables categóricas no requieren ser transformadas, solamente hay que indicar cuáles son y el algoritmo se encargará de tratarlas como tales.\n",
        "\n",
        "Puedes consultar la siguiente liga de la documentación:\n",
        "\n",
        "https://kprototypes.readthedocs.io/en/latest/api.html"
      ],
      "metadata": {
        "id": "F8R99EW72Egv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apliquemos el método de KMeans para datos mixtos (numéricos y categóricos)\n",
        "\n",
        "#                                                            ...esta celda tarda 45 mins aprox en Google-Colab ...\n",
        "\n",
        "ssecat = []    # Suma del cuadrado de los errores numéricos y categóricos .\n",
        "\n",
        "k_list = range(1, 11)\n",
        "\n",
        "cat_cols = [5,6,7,8]  # cat_list\n",
        "\n",
        "for k in k_list:\n",
        "    kp = KPrototypes(n_clusters=k, init='Huang', n_init=5)    # Inicializamos KMeans-con-varaibles-Mixtas\n",
        "    kp.fit(df_Tnum , categorical=cat_cols)     # debemos indicarle cuáles son las variables categóricas.\n",
        "    ssecat.append([k, kp.cost_])   # Calculamos la incercia de cada dato, numérica y categórica conjunta.\n",
        "\n",
        "\n",
        "oca_results_scale = pd.DataFrame({'Cluster': range(1,11), 'SSE-CAT': ssecat})\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(pd.DataFrame(ssecat)[0], pd.DataFrame(ssecat)[1], marker='o')\n",
        "plt.title('Método del Codo con datos Numéricos y Categóricos')\n",
        "plt.xlabel('Número de clusters')\n",
        "plt.ylabel('Inercia')"
      ],
      "metadata": {
        "id": "8uRwTlR1b6Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio - 1:**"
      ],
      "metadata": {
        "id": "KixS_2xuHGYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona el valor del entero \"k\" de kMeans, la cantidad de subgrupos o\n",
        "# cúmulos (clusters) que creas adecuado siguiendo la técnica del codo\n",
        "# y de acuerdo a la gráfica obtenida en la celda anterior:\n",
        "\n",
        "\n",
        "# ************* Inlcuye aquí tu código:*****************************\n",
        "\n",
        "\n",
        "k_clusters_KProto = None\n",
        "\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ],
      "metadata": {
        "id": "KnGsoo6cHLep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos que el Autoencoder es una red neuronal artificial, por lo que ahora sí requerimos transformar también las variables categóricas."
      ],
      "metadata": {
        "id": "0CRVzSS18hxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escalando la variable Ordinal:\n",
        "\n",
        "df_Tnumord = df_Tnum.copy()\n",
        "\n",
        "ordenc = OrdinalEncoder()\n",
        "df_Tnumord[['salary']] = ordenc.fit_transform(df_Tnumord[['salary']])\n",
        "\n",
        "df_Tnumord.head().T"
      ],
      "metadata": {
        "id": "SG1N0Ye8YKDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las variables binarias las dejaremos como tales y la nominal aplicaremos one-hot-encoding con el método get_dummies() de Pandas:"
      ],
      "metadata": {
        "id": "axrv9gcAJfL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Tnumordnom = df_Tnumord.copy()   # transformemos la variable Nominal con las Dummies de Pandas.\n",
        "\n",
        "X_Tnumordnom = pd.get_dummies(X_Tnumordnom, columns=['sales'], drop_first=True)\n",
        "\n",
        "df_Tnumordnom = pd.DataFrame(X_Tnumordnom)\n",
        "\n",
        "print(df_Tnumordnom.shape)\n",
        "df_Tnumordnom.head().T"
      ],
      "metadata": {
        "id": "IidlDSwLhCf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo AUTOENCODER**"
      ],
      "metadata": {
        "id": "lb5zRxN-yeTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definamos una arquitectura Autoencoder de manera usual para el caso de\n",
        "# búsqueda de conglomerados: iniciar en el Encoder incrementando las primeras\n",
        "# capas densas ocultas para deshacer lo mayor posible la dependencia de los\n",
        "# factores, para posteriormente proyectarlos al espacio latente Z de menor\n",
        "# dimensión. Cuando decimos de menor dimensión, es con respecto a la dimensión\n",
        "# de los factores de entrada. En particular usaremos aproximadamente la mitad\n",
        "# de los nodos/factores de entrada (que son 17).\n",
        "# Posteriormente el Decoder será un espejo del\n",
        "# Encoder, salvo alguna capa que podemos eliminar para que no se tenga una\n",
        "# simetría completa y obtengamos algo más que simplemente vectores muy\n",
        "# similares a los iniciales de entrada.\n",
        "# Cada capa densa la inicializamos con Xavier-Glorot y usamos ReLU como\n",
        "# función de activación.\n",
        "# Finalmente usamos Adam como optimizador y la suma de cuadrados para la\n",
        "# función de costo.\n",
        "\n",
        "input_factors = df_Tnumordnom.shape[1]  # cantidad de factores en los datos de entrada.\n",
        "int_z_latent = 8     # Dimensión del espacio latente Z.\n",
        "encoding_dim = k_clusters_KProto  # valor k que determinaste en el método del codo del Ejercicio 1.\n",
        "\n",
        "input_df = Input(shape=(input_factors, ))\n",
        "\n",
        "# Capas del ENCODER:\n",
        "x = Dense(encoding_dim, activation='relu')(input_df)\n",
        "x = Dense(400, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "x = Dense(400, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "x = Dense(1500, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "# ESPACIO LATENTE Z:\n",
        "encoded = Dense(int_z_latent, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\n",
        "\n",
        "# Capas del DECODER:\n",
        "x = Dense(1500, activation = 'relu', kernel_initializer = 'glorot_uniform')(encoded)\n",
        "x = Dense(400, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\n",
        "\n",
        "decoded = Dense( input_factors, kernel_initializer = 'glorot_uniform')(x)\n",
        "\n",
        "# Generamos los modelos Autoencoder y Encoder:\n",
        "autoencoder = Model(input_df, decoded)  # Modelo Autoencoder: (entrada de datos iniciales) + (Encoder) + (espacio-Z-latente) + (Decoder).\n",
        "encoder = Model(input_df, encoded)      # Modelo Encoder:  (entrada de datos iniciales) + (Encoder) + (espacio-Z-latente).\n",
        "\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "# Pasamos al proceso de entrenamiento del Autoencoder.\n",
        "# Aunque solo requeriremos en este ejercicio el Encoder, en el proceso debemos\n",
        "# también generar el Decoder:\n",
        "\n",
        "autoencoder.fit(df_Tnumordnom, df_Tnumordnom, batch_size=256, epochs = 30, verbose = 1)\n"
      ],
      "metadata": {
        "id": "vITn0OQVKGnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_Z_latente = encoder.predict(df_Tnumordnom)   # generamos los vectores en el espacio latente Z a la salida del Encoder."
      ],
      "metadata": {
        "id": "Zuo-LnCDlO8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_Z_latente.shape   # dimensión de los vectores en el espacio latente"
      ],
      "metadata": {
        "id": "2_iEfpRBnCYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, con base a los vectores latentes obtenidos con el Autoencoder, apliquemos kMeans (con el método usual para variables numéricas) para seleccionar cuántos cúmulos podríamos generar con estos vectores para obtener información sobre los empleados de la empresa y su potencial salida de la misma."
      ],
      "metadata": {
        "id": "ND4K1rNnRdi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_Z = []                                          #    tarda menos de 1 min\n",
        "\n",
        "range_values = range(1,20)\n",
        "\n",
        "for i in range_values:\n",
        "    kmeans = KMeans(n_clusters = i, n_init=10)\n",
        "    kmeans.fit(pred_Z_latente)\n",
        "    scores_Z.append(kmeans.inertia_)   # WCSS : within-cluster sum of square\n",
        "\n",
        "\n",
        "plt.plot(range_values, scores_Z, 'bx-')\n",
        "plt.title(\"Encontrar el número óptimo de clusters - Encoder\")\n",
        "plt.xlabel(\"Número de Clusters\")\n",
        "plt.ylabel(\"WCSS(k)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fVIUa3NGnMdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basándonos en el método del codo, podríamos decir que 5, 6 o 7 cúmulos serían adecuados.\n",
        "\n",
        "Veamos que valor de K obtenemos con la técnica Silhouette.\n",
        "\n",
        "Si no conoces esta técnica, puedes consultar la documentación correspondiente en la siguiente liga:\n",
        "\n",
        "https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html\n"
      ],
      "metadata": {
        "id": "Ic9pF8OwoFkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer"
      ],
      "metadata": {
        "id": "dusqjncNoVSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(3, 2, figsize=(10,6))            # tarda se 1 a 2 mins\n",
        "\n",
        "for i in [4,5,6,7,8,9]:\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=7)\n",
        "    q, mod = divmod(i-2, 2)\n",
        "\n",
        "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
        "    visualizer.fit(pred_Z_latente)"
      ],
      "metadata": {
        "id": "B140HYCpohfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio - 2:**"
      ],
      "metadata": {
        "id": "c_Vbt1USQTi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona el valor del entero \"k\" del número de cúmulos con base a lo\n",
        "# observado los gráficoa de la técnica del codo y de silhouette.\n",
        "# Observa que no necesriamente deben coincidir los valores observados\n",
        "# por ambos métodos, por lo que deberás elegir el que consideres\n",
        "# más adecuado:\n",
        "\n",
        "\n",
        "# ************* Inlcuye aquí tu código:*****************************\n",
        "\n",
        "\n",
        "k_clusters_Z_latente = None\n",
        "\n",
        "\n",
        "\n",
        "# *********** Aquí termina la sección de agregar código *************\n"
      ],
      "metadata": {
        "id": "HSbYBLSpQg7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km_Z_latent = KMeans(n_clusters= k_clusters_Z_latente,    # valor que seleccionaste en el Ejercicio 2.\n",
        "                     n_init=10,    # número de veces que selecciona vectores de inicialización y se queda con el de menor inercia.\n",
        "                     max_iter=500,\n",
        "                     init='k-means++').fit(pred_Z_latente)\n",
        "\n",
        "labels_Z_latent = km_Z_latent.labels_    # etiquetas para cada registro-renglón de los datos de entrada."
      ],
      "metadata": {
        "id": "qZa0Kyu7ppsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(labels_Z_latent).value_counts()   # Tamaño de los clusters obtenidos con el método del Autoencoder."
      ],
      "metadata": {
        "id": "-PHNJ213rAkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apliquemos PCA ahora a los vectores latentes obtenidos para proyectarlos y\n",
        "# visualizarlos en un espacio de dimensión 2:\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "princ_comp = pca.fit_transform(pred_Z_latente)\n",
        "pca_df = pd.DataFrame(data = princ_comp, columns=[\"pca1\", \"pca2\"])\n",
        "pca_df = pd.concat([pca_df, pd.DataFrame({\"cluster\":labels_Z_latent})], axis = 1)\n",
        "pca_df.head(3)"
      ],
      "metadata": {
        "id": "F1L8cBYwrMHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "ax = sns.scatterplot(x=\"pca1\", y = \"pca2\", hue=\"cluster\", data = pca_df,\n",
        "                     alpha=0.6,\n",
        "                     palette=[\"red\", \"green\", \"blue\",\"cyan\", \"black\",'pink','orange', \"yellow\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uSFBOLB6r85N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenamos la información de las etiquetas de cada registro del dataframe de\n",
        "# entrada indicando además en qué cúmulo se encuentran.\n",
        "# Es importante tener los valores iniciales sin transformar con la información\n",
        "# de a qué cumulo pertenece para poder darles un mejor significado desde el\n",
        "# punto de vista del negocio, en este caso sobre cada factor y su impacto en\n",
        "# la posible salida de un empleado.\n",
        "\n",
        "cluster_Z_latent_perfiles = pd.concat([dftodos, pd.DataFrame({'autoencoder_clusters':labels_Z_latent})], axis=1 )\n",
        "\n",
        "cluster_Z_latent_perfiles.head().T"
      ],
      "metadata": {
        "id": "ZFYOu0_G7aVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA - Análisis de Componentes Principales**\n",
        "\n"
      ],
      "metadata": {
        "id": "VTaNR59rymiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicaremos ahora la técnica PCA a los datos iniciales para generar igualmente cúmulos y compararlo con lo obtenido con la técnica de Autoencoder previamente vista. Esta técnica es más conocida y generalemente la que se aplica en primera instancia en un problema de agrupamiento (clustering).\n",
        "\n",
        "En este caso lo haremos tomando directamente las decisiones sobre los gráficos que estarmos generando:"
      ],
      "metadata": {
        "id": "xE62wEJc0NsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consideremos el DataFrame con los 17 vectores transformados\n",
        "# y apliquemos PCA con todos ellos para ver qué cantidad de\n",
        "# componenes principales podrían ser suficientes para un análisis\n",
        "# de relación factores y cúmulos que formaremos:\n",
        "\n",
        "pca = PCA(n_components= df_Tnumordnom.shape[1], random_state=5)\n",
        "pca.fit(df_Tnumordnom)\n",
        "\n",
        "variance = pca.explained_variance_ratio_\n",
        "var = np.cumsum(np.round(variance, 3)*100)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.ylabel('% Varianza Explicada')\n",
        "plt.xlabel('Número de Componentes Principales')\n",
        "plt.title('Anáisis PCA')\n",
        "plt.ylim(0,100.1)\n",
        "plt.plot(var);"
      ],
      "metadata": {
        "id": "FnHm36YsymOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Vemos que con 4 o 6 componentes principales se explica aproximadamente entre el 80% y 90% de su varianza.**\n",
        "\n",
        "#### **Selecconaremos el valor de 6 componentes principales (que implica cerca del 90% de la variabilidad de los datos) para fines de este ejercicio y a continuación veamos cuántos cúmulos podrían ser adecuados usar al aplicar el método de conglomerados (clustering).**"
      ],
      "metadata": {
        "id": "MUYTY7tMz_hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                            #  tarda menos de 1 min\n",
        "pca = PCA(n_components=6)    # seleccionamos 6 componentes principales que tienen un 90% de la varibilidad total\n",
        "pca_T = pca.fit_transform(df_Tnumordnom)\n",
        "pca_df_T = pd.DataFrame(pca_T, columns=['pc1','pc2','pc3','pc4','pc5','pc6'])\n",
        "\n",
        "\n",
        "sse = []\n",
        "k_list = range(1, 15)\n",
        "for k in k_list:\n",
        "    km = KMeans(n_clusters=k,n_init=7,random_state=7)\n",
        "    km.fit(pca_df_T)\n",
        "    sse.append([k, km.inertia_])\n",
        "\n",
        "pca_results_scale = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o')\n",
        "plt.title('Método del codo para Agrupamiento (Clustering) usando PCA en los datos de entrada')\n",
        "plt.xlabel('Número de cúmulos')\n",
        "plt.ylabel('Inercia')"
      ],
      "metadata": {
        "id": "zjn6OLc0ymLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos también el método Silhouette a ver que nos dice:\n",
        "\n",
        "fig, ax = plt.subplots(3, 2, figsize=(10,6))                            # tarda como 1 min\n",
        "\n",
        "for i in [4,5,6,7,8,9]:\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=7)\n",
        "    q, mod = divmod(i-2, 2)\n",
        "\n",
        "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
        "    visualizer.fit(pca_df_T)"
      ],
      "metadata": {
        "id": "0kiMe12h1MN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De ambos análisis/gráficos observamos varias opciones, pero para fines de este ejercicio seleccionaremos el valor de 5 para la cantidad de cúmulos que consideamos más adecuada con estos 6 vectores de componentes principales que generamos."
      ],
      "metadata": {
        "id": "Huwy2Dy_2pqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_pca_T = KMeans(n_clusters=5, n_init=7, max_iter=400, init='k-means++').fit(pca_df_T)\n",
        "\n",
        "labels_pca_T = kmeans_pca_T.labels_\n",
        "\n",
        "clusters_pca_T = pd.concat([pca_df_T, pd.DataFrame({'pca_clusters':labels_pca_T})], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Grafiquemos las primeras 2 componentes principales del método PCA :\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.scatterplot(x=clusters_pca_T['pc1'],\n",
        "                y=clusters_pca_T['pc2'],\n",
        "                hue=labels_pca_T,\n",
        "                palette='Set1',\n",
        "                s=100,\n",
        "                alpha=0.8\n",
        "                ).set_title('KMeans Clusters Derived from PCA', fontsize=15)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q8ksN62w1MK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenamos ahora el dataFrame de datos originales con los cúmulos identificados\n",
        "# mediante las componentes principales de los datos de entrada:\n",
        "\n",
        "cluster_pca_perfiles = pd.concat([dftodos, pd.DataFrame(clusters_pca_T['pca_clusters'])], axis=1 )\n",
        "cluster_pca_perfiles.head().T"
      ],
      "metadata": {
        "id": "ltXPYumR3fXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Relaciones entre factores de acuerdo a los cúmulos**"
      ],
      "metadata": {
        "id": "lMzXPxVA_d8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fact1 = 'last_evaluation'\n",
        "fact2 = 'satisfaction_level'\n",
        "\n",
        "mi_scatterplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "uxw7dioGtYCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box = (cluster_Z_latent_perfiles['last_evaluation'] < 0.56 ) & (cluster_Z_latent_perfiles['last_evaluation'] > 0.44) & (cluster_Z_latent_perfiles['satisfaction_level'] < 0.47 ) & (cluster_Z_latent_perfiles['satisfaction_level'] > 0.36)\n",
        "\n",
        "cluster_Z_latent_perfiles[box]"
      ],
      "metadata": {
        "id": "ZzaOStmR7Qt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio - 3:**"
      ],
      "metadata": {
        "id": "SpzSUY0I8YqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Con base a los gráficos dados en los siguientes pares de factores y en relación a los subgrupos o cúmulos observados por el método de Autoencoders y el de PCA, incluye alguna interpretación del mismo que ayude a entender mejor al departamento de recursos humanos el ambiente de trabajo dentro de la empresa.**"
      ],
      "metadata": {
        "id": "xwlpdz359Opx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3a:\n",
        "\n",
        "\n",
        "fact1 = 'last_evaluation'\n",
        "fact2 = 'average_montly_hours'\n",
        "\n",
        "mi_scatterplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "rMazNNOpAdCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3a:**"
      ],
      "metadata": {
        "id": "o0Q9KMddEcB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "63GvIfmvEkg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3b:\n",
        "\n",
        "\n",
        "fact1 = 'number_project'\n",
        "fact2 = 'satisfaction_level'\n",
        "\n",
        "mi_catplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "CurueJVSEAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3b:**"
      ],
      "metadata": {
        "id": "cmmxH-BTEzq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "XDnPIfaZExs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3c:\n",
        "\n",
        "\n",
        "fact1 = 'Work_accident'\n",
        "fact2 = 'satisfaction_level'\n",
        "\n",
        "mi_catplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "K_ap8l2SFEuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3c:**"
      ],
      "metadata": {
        "id": "Qb_QMZZUFtYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "C-JMzlOiFtNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3d:\n",
        "\n",
        "fact1 = 'number_project'\n",
        "fact2 = 'last_evaluation'\n",
        "\n",
        "mi_catplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "uA7ui6KRFg1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3d:**"
      ],
      "metadata": {
        "id": "AGPZ7A5BGCvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "AZItVuKtGBrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3e:\n",
        "\n",
        "fact2 = 'sales'\n",
        "fact1 = 'satisfaction_level'\n",
        "\n",
        "mi_catplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "mYGuvOalHY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3e:**"
      ],
      "metadata": {
        "id": "vLoF3ZhTGvmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "w8B27uYiGyqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3f:\n",
        "\n",
        "\n",
        "fact1 = 'salary'\n",
        "fact2 = 'average_montly_hours'\n",
        "\n",
        "mi_catplotPCAEncoder(cluster_Z_latent_perfiles, cluster_pca_perfiles, fact1, fact2)"
      ],
      "metadata": {
        "id": "Ea-yBXKQIX0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3f:**"
      ],
      "metadata": {
        "id": "q-Huc1IGHP2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "pS7EQT4BHSDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3g:**"
      ],
      "metadata": {
        "id": "xXyxzjrVxJ9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluye algún otro gráfico y tus comentarios de otra relación entre variables\n",
        "# que consideres proporcione información importante al problema.\n",
        "\n",
        "\n",
        "# ******************** Incluuye a continuación tu código **********************\n",
        "\n",
        "None\n",
        "\n",
        "\n",
        "# *********************** Termina sección para agregar código *****************\n",
        "\n"
      ],
      "metadata": {
        "id": "CjRC_DYex5Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener algunos diagramas adicionales que nos ayuden a entender\n",
        "# mejor cada cúmulo y su relación con los factores que la forman.\n",
        "# Sabemos que exiten una gran variedad de otros tipos e gráficos.\n",
        "# En particular podemos mostrar los de los cúmulos obtenidos con el método\n",
        "# de autoencoders solo como muestra y a manera de complemento de la\n",
        "# información que hemos obtenida con los gráficos anteriores:\n",
        "\n",
        "for c in cluster_pca_perfiles.iloc[:,:9]:\n",
        "    grid = sns.FacetGrid(cluster_pca_perfiles, col='pca_clusters')\n",
        "    grid.map(plt.hist, c)"
      ],
      "metadata": {
        "id": "ZQA6Eb_jCv4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio - 4:**"
      ],
      "metadata": {
        "id": "Aq5YiOtuZ8ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Incluye tus conclusiones y comentarios finales de la Actividad:**\n",
        "\n",
        "++++++++ Inicia la sección de agregar texto: +++++++++++\n",
        "\n",
        "None\n",
        "\n",
        "++++++++ Termina la sección de agregar texto. +++++++++++"
      ],
      "metadata": {
        "id": "cmsBGuNHaDV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> **++++ Fin del la Actividad de la Semana de Autoencoders ++++**"
      ],
      "metadata": {
        "id": "5Wwjsvuo883U"
      }
    }
  ]
}